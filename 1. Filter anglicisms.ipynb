{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, xmljson #, xmltodict\n",
    "import pickle\n",
    "from lxml.etree import parse, fromstring, tostring\n",
    "from xmljson import badgerfish as bf\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import pandas as pd, networkx as nx\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"yes\"?>\n",
    "# Please remove fist line and the end sybmol(wtfit) in Posts.xml\n",
    "with open('../Posts.xml','r',encoding=\"utf-8-sig\") as f:\n",
    "    xml_raw = f.read()\n",
    "    # doc = xmltodict.parse(xml_raw)\n",
    "    data = bf.data(fromstring(xml_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ld = []\n",
    "\n",
    "# for i, d in enumerate(data[\"documents\"][\"document\"]):\n",
    "#     dd = dict()\n",
    "#     for key, value in d.items():\n",
    "#         # print(key, i)\n",
    "#         if key in ['isRepost', 'TotalVotes', 'PostGeoLocation','Parent', 'RootPostID']:\n",
    "#             dd[key] = value\n",
    "#         elif key == \"OriginalPost\":\n",
    "#             #TODO not to remove original post\n",
    "#             pass\n",
    "#         #TODO make time preprocessing\n",
    "#         else:\n",
    "#             try:\n",
    "#                 dd[key] = value['$']\n",
    "#             except KeyError:\n",
    "#                 print(\"line: \", i, \"key: \", key, \"value: \", value)\n",
    "#                 dd[key] = value\n",
    "#     ld.append(dd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = []\n",
    "\n",
    "for i, d in enumerate(data[\"documents\"][\"document\"]):\n",
    "    dd = dict()\n",
    "    # Remove empty texts\n",
    "    try:\n",
    "        for key, value in d.items():\n",
    "            # print(key, i)\n",
    "            if key in ['isRepost', 'TotalVotes', 'PostGeoLocation','Parent', 'RootPostID']:\n",
    "                dd[key] = value\n",
    "            elif key == \"OriginalPost\":\n",
    "                #TODO not to remove original post\n",
    "                pass\n",
    "            #TODO make time preprocessing\n",
    "            else:\n",
    "                dd[key] = value['$']\n",
    "                \n",
    "    except KeyError:\n",
    "        continue\n",
    "    ld.append(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[46738,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(46738,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ID', 'AuthorID', 'OwnerID',\n",
    "     'Text', 'URL', 'CreatedDate', 'PostType','AuthorName', 'LikeCount',\n",
    "     'ShareCount', 'UShares', 'References', 'ViewsCount',\n",
    "     'spamWeight', 'Photos', 'Videos', 'Documents',\n",
    "     'Audios', 'Mentions']].to_csv(\"../post_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base text simularity\n",
    "# code: https://medium.com/better-programming/introduction-to-gensim-calculating-text-similarity-9e8b55de342d\n",
    "# BERT http://docs.deeppavlov.ai/en/master/features/pretrained_vectors.html\n",
    "# code: https://github.com/huggingface/transformers\n",
    "# rename bert_config.json to config.json\n",
    "pretrained_weights = \"../ru_conversational_cased_L-12_H-768_A-12_pt/\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "model = BertModel.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_tok = []\n",
    "# for i, text in enumerate(df['Text']):\n",
    "#     try:\n",
    "#         text_tok.append(tokenizer.encode(text, add_special_tokens=True))\n",
    "#     except ValueError:\n",
    "#         print(i, text)\n",
    "text_tok = [tokenizer.encode(text, add_special_tokens=True) for text in df['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../text_tok.pkl', 'wb') as f:\n",
    "#     pickle.dump(obj=text_tok, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../text_tok.pkl', 'rb') as f:\n",
    "    text_tok = pickle.load(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def it(text):\n",
    "    return text\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=it, lowercase=False)    \n",
    "tfidf_tok = tfidf.fit_transform(text_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm.notebook import tqdm\n",
    "from numba import njit, prange\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tok_q = tfidf_tok.astype(dtype = np.float32, casting='unsafe', copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = tfidf_tok_q.shape[0]\n",
    "# sim_matrix = np.empty(shape=(n,n))\n",
    "\n",
    "# for i, row in tqdm(enumerate(tfidf_tok_q)):\n",
    "#     for j, col in tqdm(enumerate(tfidf_tok_q)):\n",
    "#         # print(i, j)\n",
    "#         sim_matrix[i, j] = cosine(u=row.toarray(), v=col.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tok_q = tfidf_tok_q.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(arr):\n",
    "    n = arr.shape[0]\n",
    "    sim_matrix = np.empty(shape=(n,n))\n",
    "    @njit(parallel=True)\n",
    "    def in_cycle(row):\n",
    "        sim_row = np.zeros(shape=n)\n",
    "        for j in prange(n):\n",
    "            col = arr[j, :]\n",
    "            uv = row * col\n",
    "            uu = row * row\n",
    "            vv = col * col\n",
    "            sim_row[j] = uv.sum()/np.sqrt(uu.sum()*vv.sum())\n",
    "        return sim_row\n",
    "    for i in tqdm(range(n)):\n",
    "        #print(i)\n",
    "        row = arr[i, :]\n",
    "        sim_matrix[i, :] = np.array(in_cycle(row))\n",
    "    return sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim_matrix = tfidf_tok_q.dot(tfidf_tok_q.T)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarities = cosine_similarity(tfidf_tok_q)\n",
    "# sim_matrix = cosine_sim(arr=tfidf_tok_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_query = gensim.similarities.SparseMatrixSimilarity(corpus=tfidf_tok, num_features=tfidf_tok.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode text\n",
    "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0]  # Models outputs are now tuples\n",
    "\n",
    "# Each architecture is provided with several class for fine-tuning on down-stream tasks, e.g.\n",
    "BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n",
    "                      BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering]\n",
    "\n",
    "# All the classes for an architecture can be initiated from pretrained weights for this architecture\n",
    "# Note that additional weights added for fine-tuning are only initialized\n",
    "# and need to be trained on the down-stream task\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "\n",
    "for model_class in BERT_MODEL_CLASSES:\n",
    "    # Load pretrained model/tokenizer\n",
    "    model = model_class.from_pretrained(pretrained_weights)\n",
    "\n",
    "    # Models can return full list of hidden-states & attentions weights at each layer\n",
    "    model = model_class.from_pretrained(pretrained_weights,\n",
    "                                        output_hidden_states=True,\n",
    "                                        output_attentions=True)\n",
    "    input_ids = torch.tensor([tokenizer.encode(\"Let's see all hidden-states and attentions on this text\")])\n",
    "    all_hidden_states, all_attentions = model(input_ids)[-2:]\n",
    "\n",
    "    # Models are compatible with Torchscript\n",
    "    model = model_class.from_pretrained(pretrained_weights, torchscript=True)\n",
    "    traced_model = torch.jit.trace(model, (input_ids,))\n",
    "\n",
    "    # Simple serialization for models and tokenizers\n",
    "    model.save_pretrained('./directory/to/save/')  # save\n",
    "    model = model_class.from_pretrained('./directory/to/save/')  # re-load\n",
    "    tokenizer.save_pretrained('./directory/to/save/')  # save\n",
    "    tokenizer = BertTokenizer.from_pretrained('./directory/to/save/')  # re-load\n",
    "\n",
    "    # SOTA examples for GLUE, SQUAD, text generation...\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
